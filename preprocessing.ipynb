{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/darienpmt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('letters_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing remove numbers, captial letters and punctuation\n",
    "\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "df.Letter = df.Letter.map(alphanumeric).map(punc_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Letter = df.Letter.str.replace('silvio', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Letter = df.Letter.str.replace('annette', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing references to the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september',\n",
    "                       'october', 'november', 'december']\n",
    "\n",
    "for month in months:\n",
    "    df.Letter = df.Letter.str.replace(month, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing references to the location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['australia', 'new guinea', 'southwest pacific', 'pacific', 'phillippine islands', 'mindanao', \n",
    "             'zamboanga', 'somewhere in the pacific', 'leyte gulf', 'okinawa', 'japan', 'hiro', 'california']\n",
    "\n",
    "for location in locations:\n",
    "    df.Letter = df.Letter.str.replace(location, '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle unstemmed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./pickles/unstemmed_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix of the plain text (no stemming or lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plain = pd.read_pickle('./pickles/unstemmed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "df_plain_cv = cv.fit_transform(df_plain.Letter)\n",
    "df_plain_dtm1 = pd.DataFrame(df_plain_cv.toarray(), columns=cv.get_feature_names())\n",
    "df_plain_dtm1.index = df_plain.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plain_dtm1.to_pickle('./pickles/plain_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stemmed_letter'] = df.Letter.apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Letter', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Stemmed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./pickles/stemmed_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = pd.read_pickle('./pickles/unstemmed_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem['Lemmatized_letter'] = df_lem.Letter.apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem.drop('Letter', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Lemmatized dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem.to_pickle('./pickles/lemmatized_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns = pd.DataFrame(df.Letter.apply(nouns))\n",
    "df_nouns.index = df.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle Nouns df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns.to_pickle('./pickles/nouns_df_rawtext.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Nouns df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_nouns['Lemmatized_letter'] = df_nouns.Letter.apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the nouns df lemmatized\n",
    "df_nouns.drop('Letter', axis=1, inplace=True)\n",
    "\n",
    "df_nouns.to_pickle('./pickles/nouns_df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjectives and Nouns DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj = pd.DataFrame(df.Letter.apply(nouns_adj))\n",
    "df_nouns_adj.index = df.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle Nouns/Adjs df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj.to_pickle('./pickles/nouns_adj__df_rawtext.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize Nouns/Adj df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_nouns_adj['Lemmatized_letter'] = df_nouns_adj.Letter.apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in x.split()]))\n",
    "\n",
    "df_nouns_adj.drop('Letter', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the nouns/adj df lemmatized\n",
    "\n",
    "df_nouns_adj.to_pickle('./pickles/nouns_adj__df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj['Location'] = np.array(df['Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make each new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_df(df, location):\n",
    "    return df[df.Location == location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make df for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_df = loc_df(df_nouns_adj, 'Australia')\n",
    "\n",
    "swp_df = loc_df(df_nouns_adj, 'Southwest Pacific')\n",
    "\n",
    "ng_df = loc_df(df_nouns_adj, 'New Guinea')\n",
    "\n",
    "pi_df = loc_df(df_nouns_adj, 'Philippine Islands')\n",
    "\n",
    "z_df = loc_df(df_nouns_adj, 'Zamboanga')\n",
    "\n",
    "j_df = loc_df(df_nouns_adj, 'Japan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle each new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dfs = [aus_df, swp_df, ng_df, pi_df, z_df, j_df]\n",
    "\n",
    "unq_locations = ['Australia', 'Southwest Pacific', 'New Guinea',\n",
    "       'Philippine Islands', 'Zamboanga', 'Japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in zip(unq_locations, loc_dfs):\n",
    "    df.to_pickle('./pickles/{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document term matrix from Stemmed text (tokenized by word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pickle.load(open('./pickles/stop_words_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(stop_words=stop_words, min_df=3, max_df=0.7 )\n",
    "\n",
    "df_cv1 = cv1.fit_transform(df.Stemmed_letter)\n",
    "df_dtm1 = pd.DataFrame(df_cv1.toarray(), columns=cv1.get_feature_names())\n",
    "df_dtm1.index = df.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm1.to_pickle('./pickles/dtm1_stemmed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document term matrix from Lemmatized text (tokenized by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(stop_words=stop_words, min_df=3, max_df=0.7 )\n",
    "\n",
    "df_lem_cv1 = cv1.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_lem_dtm1 = pd.DataFrame(df_lem_cv1.toarray(), columns=cv1.get_feature_names())\n",
    "df_lem_dtm1.index = df_lem.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_dtm1.to_pickle('./pickles/dtm1_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix from Stemmed text (tokenized by 2,3-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(2,3), stop_words=stop_words, min_df=3, max_df=0.7 )\n",
    "\n",
    "df_cv2 = cv2.fit_transform(df.Stemmed_letter)\n",
    "df_dtm2 = pd.DataFrame(df_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "df_dtm2.index = df.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtm2.to_pickle('./pickles/dtm2_stemmed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix from Lemmatized text (tokenized by 2,3-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(2,3), stop_words=stop_words, min_df=3, max_df=0.7 )\n",
    "\n",
    "df_lem_cv2 = cv2.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_lem_dtm2 = pd.DataFrame(df_lem_cv2.toarray(), columns=cv2.get_feature_names())\n",
    "df_lem_dtm2.index = df_lem.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem_dtm2.to_pickle('./pickles/dtm2_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one word\n",
    "\n",
    "tfidf1 = TfidfVectorizer(stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf1_lem = tfidf1.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_tfidf1_lem = pd.DataFrame(tfidf1_lem.toarray(), columns=tfidf1.get_feature_names())\n",
    "df_tfidf1_lem.index = df_lem.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf1_lem.to_pickle('./pickles/tfidf1_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two to three words\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(2,3), binary=True, stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf2_lem = tfidf2.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_tfidf2_lem = pd.DataFrame(tfidf2_lem.toarray(), columns=tfidf2.get_feature_names())\n",
    "df_tfidf2_lem.index = df_lem.Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf2_lem.to_pickle('./pickles/tfidf2_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF for nouns and adj dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in nouns df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns = pd.read_pickle('./pickles/nouns_df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one word\n",
    "\n",
    "tfidf1 = TfidfVectorizer(stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf1_nouns = tfidf1.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_tfidf1_nouns = pd.DataFrame(tfidf1_nouns.toarray(), columns=tfidf1.get_feature_names())\n",
    "df_tfidf1_nouns.index = df_nouns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf1_nouns.to_pickle('./pickles/tfidf1_nouns_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in nouns/adj df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj = pd.read_pickle('./pickles/nouns_adj__df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(ngram_range=(1,3), stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf2_nouns_adj = tfidf2.fit_transform(df_lem.Lemmatized_letter)\n",
    "df_tfidf2_nouns_adj = pd.DataFrame(tfidf2_nouns_adj.toarray(), columns=tfidf2.get_feature_names())\n",
    "df_tfidf2_nouns_adj.index = df_nouns_adj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf2_nouns_adj.to_pickle('./pickles/tfidf2_nouns_adj_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF for each location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_df = pd.read_pickle('./pickles/Australia.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "swp_df = pd.read_pickle('./pickles/Southwest Pacific.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_df = pd.read_pickle('./pickles/New Guinea.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_df = pd.read_pickle('./pickles/Philippine Islands.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_df = pd.read_pickle('./pickles/Zamboanga.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df = pd.read_pickle('./pickles/Japan.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dfs = [aus_df, swp_df, ng_df, pi_df, z_df, j_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_locations = ['Australia', 'Southwest Pacific', 'New Guinea',\n",
    "       'Philippine Islands', 'Zamboanga', 'Japan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the TF-IDF for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(ngram_range=(1,3), stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "for name, loc_df in zip(unq_locations, loc_dfs):\n",
    "\n",
    "    tfidf2_local = tfidf2.fit_transform(loc_df.Lemmatized_letter)\n",
    "    df_tfidf1_local = pd.DataFrame(tfidf2_local.toarray(), columns=tfidf2.get_feature_names())\n",
    "    df_tfidf1_local.index = loc_df.index\n",
    "    \n",
    "    df_tfidf1_local.to_pickle('./pickles/tfidf2_{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('./pickles/tfidf2_Australia.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
