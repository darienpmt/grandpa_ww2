{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/darienpmt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/darienpmt/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/metis/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/darienpmt/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/darienpmt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk.util import tree2conlltags\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2_lem = pd.read_pickle('./pickles/dtm2_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to display topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\"\n",
    "    Displays the top n terms in each topic\n",
    "    \"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix + 1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(dtm2_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\"],\n",
    "             columns = dtm2_lem.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "beloved wife, feeling fine, happy birthday, looking forward, philippine island\n",
      "\n",
      "Topic  2\n",
      "philippine island, looking forward, mother aunt, war end, aunt uncle\n",
      "\n",
      "Topic  3\n",
      "happy birthday, mother aunt, mother aunt uncle, aunt uncle, gentle breeze\n",
      "\n",
      "Topic  4\n",
      "mr taylor, great deal, gentle breeze, play golf, went visit\n",
      "\n",
      "Topic  5\n",
      "looking forward, far away, early morning, far away danger, away danger\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, dtm2_lem.columns, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame(doc_topic.round(5),\n",
    "             index = dtm2_lem.index,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(dtm2_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\"],\n",
    "             columns = dtm2_lem.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "beloved wife, happy birthday, little bit, feeling fine, special kiss\n",
      "\n",
      "Topic  2\n",
      "philippine island, overseas duty, war europe, discharged army, end war\n",
      "\n",
      "Topic  3\n",
      "looking forward, money order, united state, came overseas, war end\n",
      "\n",
      "Topic  4\n",
      "great deal, mr taylor, play golf, feeling fine, far away\n",
      "\n",
      "Topic  5\n",
      "mother aunt, aunt uncle, mother aunt uncle, gentle breeze, breeze blowing\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, dtm2_lem.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = pd.read_pickle('./pickles/lemmatized_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pickle.load(open('./pickles/stop_words_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2, 3),  stop_words=stop_words, \n",
    "                                   min_df=3, max_df=0.7, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "\n",
    "count_vectorizer.fit(df_lem.Lemmatized_letter)\n",
    "\n",
    "doc_word = count_vectorizer.transform(df_lem.Lemmatized_letter).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "\n",
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=5)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf2_lem = pd.read_pickle('./pickles/tfidf2_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(df_tfidf2_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(lsa.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\"],\n",
    "             columns = df_tfidf2_lem.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "beloved wife, looking forward, philippine island, feeling fine, far away\n",
      "\n",
      "Topic  2\n",
      "western union telegram, western union, union telegram, san francisco, mr tontar\n",
      "\n",
      "Topic  3\n",
      "beloved wife, wife oh, battalion surgeon, wrote wrote, feeling fine\n",
      "\n",
      "Topic  4\n",
      "looking forward, beloved wife, pursuit happiness, came overseas, duty ask\n",
      "\n",
      "Topic  5\n",
      "early morning, looking forward, far away, happiest man, send money\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, df_tfidf2_lem.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(df_tfidf2_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(3),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\"],\n",
    "             columns = df_tfidf2_lem.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "feeling fine, mother aunt, breeze blowing, far away, aunt uncle\n",
      "\n",
      "Topic  2\n",
      "western union, western union telegram, union telegram, san francisco, mr tontar\n",
      "\n",
      "Topic  3\n",
      "beloved wife, duty ask, wife oh, adore beloved, adore beloved wife\n",
      "\n",
      "Topic  4\n",
      "looking forward, early morning, wife remember, wake morning, felt bad\n",
      "\n",
      "Topic  5\n",
      "philippine island, medical officer, near future, sky high, sorry disappoint\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, df_tfidf2_lem.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(ngram_range=(2,3), binary=True, stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf2.fit(df_lem.Lemmatized_letter)\n",
    "\n",
    "doc_word = tfidf2.transform(df_lem.Lemmatized_letter).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "\n",
    "id2word = dict((v, k) for k, v in tfidf2.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with Nouns and Nouns/Adjectives only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_lem = pd.read_pickle('./pickles/nouns_df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf1_nouns = pd.read_pickle('./pickles/tfidf2_nouns_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(df_tfidf1_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "wife, feeling, happy, tell, war\n",
      "\n",
      "Topic  2\n",
      "island, jap, sea, ship, philippine\n",
      "\n",
      "Topic  3\n",
      "western, union, tontar, sea, telegram\n",
      "\n",
      "Topic  4\n",
      "western, union, tontar, telegram, francisco\n",
      "\n",
      "Topic  5\n",
      "birthday, native, tontar, weight, coconut\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, df_tfidf1_nouns.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "doc_topic = nmf_model.fit_transform(df_tfidf1_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "christmas, wife, money, send, golf\n",
      "\n",
      "Topic  2\n",
      "sea, ship, island, land, hill\n",
      "\n",
      "Topic  3\n",
      "western, union, tontar, telegram, francisco\n",
      "\n",
      "Topic  4\n",
      "war, overseas, expect, hospital, end\n",
      "\n",
      "Topic  5\n",
      "birthday, native, feeling, work, happy\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, df_tfidf1_nouns.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf1.fit(df_nouns_lem.Lemmatized_letter)\n",
    "\n",
    "doc_word = tfidf1.transform(df_nouns_lem.Lemmatized_letter).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "\n",
    "id2word = dict((v, k) for k, v in tfidf1.vocabulary_.items())\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=4, id2word=id2word, passes=10)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj_lem = pd.read_pickle('./pickles/nouns_adj__df_lemmatize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf2_nouns_adj = pd.read_pickle('./pickles/tfidf2_nouns_adj_lemmatized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(5)\n",
    "doc_topic = lsa.fit_transform(df_tfidf2_nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "wife, feeling, happy, tell, war\n",
      "\n",
      "Topic  2\n",
      "western union telegram, western union, union telegram, western, union\n",
      "\n",
      "Topic  3\n",
      "western union telegram, union telegram, western union, western, union\n",
      "\n",
      "Topic  4\n",
      "war, hospital, medical, money, expect\n",
      "\n",
      "Topic  5\n",
      "birthday, native, happy birthday, coconut, rat\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, df_tfidf2_nouns_adj.columns, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel like this would be pretty good if it weren't for that overlap seen in topics 2 and 3. Oh well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(6, random_state=0)\n",
    "doc_topic = nmf_model.fit_transform(df_tfidf2_nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "war, hospital, expect, medical, overseas, end, news, duty, officer, change\n",
      "\n",
      "Topic  2\n",
      "christmas, picture, morning, went, dinner, cold, golf, played, people, warm\n",
      "\n",
      "Topic  3\n",
      "union telegram, western union, western union telegram, western, union, tontar, telegram, francisco, san francisco, san\n",
      "\n",
      "Topic  4\n",
      "ship, sea, land, typhoon, calm, tonight, kure, morning, bay, island\n",
      "\n",
      "Topic  5\n",
      "jap, island, native, philippine, rain, philippine island, enemy, campaign, wet, landed\n",
      "\n",
      "Topic  6\n",
      "birthday, wife, beloved, send, happy, beloved wife, kiss, remember, feeling, able\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, df_tfidf2_nouns_adj.columns, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human interpretation of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think these are my most clear topics. After much thought, here is how I interpret it:\n",
    "\n",
    "1. Duty\n",
    "2. Comfort/Normalization\n",
    "3. Money\n",
    "4. Traveling at sea  \n",
    "5. Inner Conflict/Struggle\n",
    "6. Romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(nmf_model.components_.round(5),\n",
    "             index = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\",\n",
    "                     \"component_6\"],\n",
    "             columns = df_tfidf2_nouns_adj.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame(doc_topic.round(5),\n",
    "             index = df_tfidf2_nouns_adj.index,\n",
    "             columns = [\"component_1\",\"component_2\", \"component_3\",\"component_4\", \"component_5\",\n",
    "                       \"component_6\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic.index = pd.to_datetime(doc_topic.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_qrts = doc_topic.resample('3M').mean()\n",
    "\n",
    "topics = doc_topic_qrts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_qrts.columns = ['Duty', 'Comfort& Normalization', 'Money' , 'Traveling at Sea',\n",
    "                          'Inner Conflict & Struggle', 'Romance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_qrts.drop('Money', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this dataframe to csv for tableau visualization\n",
    "\n",
    "doc_topic_qrts.to_csv('Letter_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "for topic in topics:\n",
    "    plt.plot(doc_topic_qrts[topic])\n",
    "\n",
    "plt.title('Topics seen though Time', fontsize=20, weight='bold', pad=20)\n",
    "plt.xlabel('Date', fontsize=15, weight='bold',labelpad=10)\n",
    "plt.ylabel('Prevelence of Topic', fontsize=15,weight='bold',labelpad=10)\n",
    "plt.legend(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find letter dates which are most representative of a particular topic\n",
    "\n",
    "doc_topic.component_4.sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a basic recommender system (not sure if this makes sense here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nouns_adj = pd.read_pickle('./pickles/nouns_adj__df_lemmatize.pkl')\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,3), stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf2_nouns_adj = tfidf2.fit_transform(df_lem.Lemmatized_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [\"the war is over\"]\n",
    "\n",
    "vt = tfidf2.transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = nmf_model.transform(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances(tt,doc_topic,metric='cosine').argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1945-07-31 00:00:00')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic.index[268]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(ngram_range=(2,3), binary=True, stop_words=stop_words, min_df=3, max_df=0.7)\n",
    "\n",
    "tfidf2.fit(df_nouns_adj_lem.Lemmatized_letter)\n",
    "\n",
    "doc_word = tfidf2.transform(df_nouns_adj_lem.Lemmatized_letter).transpose()\n",
    "\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "\n",
    "id2word = dict((v, k) for k, v in tfidf2.vocabulary_.items())\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=5, id2word=id2word, passes=10)\n",
    "\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probably does not work as well since the documents are rather small. It makes sense that NMF (or LSA) would give more clear results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling corpus by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_locations = ['Australia', 'Southwest Pacific', 'New Guinea',\n",
    "       'Philippine Islands', 'Zamboanga', 'Japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_aus = pd.read_pickle('./pickles/tfidf2_Australia.pkl')\n",
    "\n",
    "tfidf_swp = pd.read_pickle('./pickles/tfidf2_Southwest Pacific.pkl')\n",
    "\n",
    "tfidf_ng = pd.read_pickle('./pickles/tfidf2_New Guinea.pkl')\n",
    "\n",
    "tfidf_pi = pd.read_pickle('./pickles/tfidf2_Philippine Islands.pkl')\n",
    "\n",
    "tfidf_zam = pd.read_pickle('./pickles/tfidf2_Zamboanga.pkl')\n",
    "\n",
    "tfidf_japan = pd.read_pickle('./pickles/tfidf2_Japan.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = [tfidf_aus, tfidf_swp, tfidf_ng, tfidf_pi, tfidf_zam, tfidf_japan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n",
      "\n",
      "Topic  1\n",
      "wife, birthday, golf, able, mother\n",
      "\n",
      "Topic  2\n",
      "tontar, western union, western union telegram, western, union telegram\n",
      "\n",
      "Topic  3\n",
      "school, trip, camp, train, course\n",
      "\n",
      "Topic  4\n",
      "christmas, picture, child, morning, holiday\n",
      "\n",
      "\n",
      "Southwest Pacific\n",
      "\n",
      "Topic  1\n",
      "christmas, money, tonight, thing, island\n",
      "\n",
      "Topic  2\n",
      "native, child, sea, hut, patient\n",
      "\n",
      "Topic  3\n",
      "news, coffee, cup, meal, sheet\n",
      "\n",
      "Topic  4\n",
      "book, people, jap, great, girl\n",
      "\n",
      "\n",
      "New Guinea\n",
      "\n",
      "Topic  1\n",
      "native, air, picture, book, husband\n",
      "\n",
      "Topic  2\n",
      "rat, trap, rat trap, chaplain, man\n",
      "\n",
      "Topic  3\n",
      "birthday, happy birthday, food, wonderful, happy\n",
      "\n",
      "Topic  4\n",
      "wife, able, anniversary, people, past\n",
      "\n",
      "\n",
      "Philippine Islands\n",
      "\n",
      "Topic  1\n",
      "jap, point, campaign, rain, mud\n",
      "\n",
      "Topic  2\n",
      "war, happy, news, york, book\n",
      "\n",
      "Topic  3\n",
      "morning, native, care, hot, warm\n",
      "\n",
      "Topic  4\n",
      "dry, weather, men, wind, people\n",
      "\n",
      "\n",
      "Zamboanga\n",
      "\n",
      "Topic  1\n",
      "hospital, world, medical, officer, doctor\n",
      "\n",
      "Topic  2\n",
      "future, philippine island, island, quiet, philippine\n",
      "\n",
      "Topic  3\n",
      "sure, work, great, usual, trip\n",
      "\n",
      "Topic  4\n",
      "morale, john, tent, alive, news\n",
      "\n",
      "\n",
      "Japan\n",
      "\n",
      "Topic  1\n",
      "mile, men, shima, ese, bomb\n",
      "\n",
      "Topic  2\n",
      "typhoon, ship, lot, sea, bay\n",
      "\n",
      "Topic  3\n",
      "land, morning, island, anxious, kure\n",
      "\n",
      "Topic  4\n",
      "dream, end, heart, tomorrow, order\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, tfidf in zip(unq_locations, locations):\n",
    "    \n",
    "    nmf_model = NMF(4)\n",
    "    doc_topic = nmf_model.fit_transform(tfidf)\n",
    "    \n",
    "    print(name)\n",
    "    display_topics(nmf_model, tfidf.columns, 5)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, but maybe a bit too much for my purposes right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
